{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2c6PyqQaNiA"
      },
      "source": [
        "# Using the Language Interpretability Tool in Notebooks\n",
        "\n",
        "This notebook shows use of the [Language Interpretability Tool](https://pair-code.github.io/lit) on a binary classifier for labelling statement sentiment (0 for negative, 1 for positive).\n",
        "\n",
        "The LitWidget object constructor takes a dict mapping model names to model objects, and a dict mapping dataset names to dataset objects. Those will be the datasets and models displayed in LIT. It also optionally takes in a `height` parameter for how tall to render the LIT UI in pixels (it defaults to 1000 pixels). Running the constructor will cause the LIT server to be started in the background, loading the models and datasets and enabling the UI to be served.\n",
        "\n",
        "Render the LIT UI in an output cell by calling the `render` method on the LitWidget object. The LIT UI can be rendered multiple times in separate cells if desired. The widget also contains a `stop` method to shut down the LIT server.\n",
        "\n",
        "Copyright 2020 Google LLC.\n",
        "SPDX-License-Identifier: Apache-2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ukXamAB_FBM8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: tensorflow-datasets 4.7.0\n",
            "Uninstalling tensorflow-datasets-4.7.0:\n",
            "  Successfully uninstalled tensorflow-datasets-4.7.0\n",
            "Collecting lit_nlp\n",
            "  Using cached lit_nlp-0.5.0-py3-none-any.whl (12.3 MB)\n",
            "Collecting tfds-nightly\n",
            "  Using cached tfds_nightly-4.7.0.dev202212100044-py3-none-any.whl (5.1 MB)\n",
            "Collecting transformers==4.1.1\n",
            "  Using cached transformers-4.1.1-py3-none-any.whl (1.5 MB)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Using cached tokenizers-0.9.4.tar.gz (184 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.1.1) (2022.8.17)\n",
            "Requirement already satisfied: numpy in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.1.1) (1.23.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.1.1) (3.8.0)\n",
            "Requirement already satisfied: requests in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.1.1) (2.28.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\asif.raza\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.1.1) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.1.1) (4.64.1)\n",
            "Collecting sacremoses\n",
            "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
            "Requirement already satisfied: portpicker in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lit_nlp) (1.5.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lit_nlp) (1.1.2)\n",
            "Collecting umap-learn\n",
            "  Using cached umap_learn-0.5.3-py3-none-any.whl\n",
            "Requirement already satisfied: absl-py in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lit_nlp) (1.2.0)\n",
            "Requirement already satisfied: sacrebleu in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lit_nlp) (2.3.1)\n",
            "Collecting ml-collections\n",
            "  Using cached ml_collections-0.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: pandas in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lit_nlp) (1.4.4)\n",
            "Requirement already satisfied: Werkzeug in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lit_nlp) (2.2.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lit_nlp) (1.9.1)\n",
            "Requirement already satisfied: attrs in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lit_nlp) (22.1.0)\n",
            "Requirement already satisfied: promise in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tfds-nightly) (2.3)\n",
            "Requirement already satisfied: dill in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tfds-nightly) (0.3.6)\n",
            "Collecting dm-tree\n",
            "  Using cached dm_tree-0.1.7-cp310-cp310-win_amd64.whl (90 kB)\n",
            "Requirement already satisfied: etils[epath] in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tfds-nightly) (0.9.0)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tfds-nightly) (3.20.3)\n",
            "Requirement already satisfied: six in c:\\users\\asif.raza\\appdata\\roaming\\python\\python310\\site-packages (from tfds-nightly) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-metadata in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tfds-nightly) (1.12.0)\n",
            "Requirement already satisfied: termcolor in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied: toml in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tfds-nightly) (0.10.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.1.1) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.1.1) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.1.1) (1.26.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.1.1) (2022.6.15)\n",
            "Requirement already satisfied: colorama in c:\\users\\asif.raza\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers==4.1.1) (0.4.5)\n",
            "Requirement already satisfied: importlib_resources in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from etils[epath]->tfds-nightly) (5.10.1)\n",
            "Requirement already satisfied: typing_extensions in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from etils[epath]->tfds-nightly) (4.3.0)\n",
            "Requirement already satisfied: zipp in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from etils[epath]->tfds-nightly) (3.11.0)\n",
            "Requirement already satisfied: PyYAML in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ml-collections->lit_nlp) (6.0)\n",
            "Collecting contextlib2\n",
            "  Using cached contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\asif.raza\\appdata\\roaming\\python\\python310\\site-packages (from packaging->transformers==4.1.1) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\asif.raza\\appdata\\roaming\\python\\python310\\site-packages (from pandas->lit_nlp) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->lit_nlp) (2022.2.1)\n",
            "Requirement already satisfied: psutil in c:\\users\\asif.raza\\appdata\\roaming\\python\\python310\\site-packages (from portpicker->lit_nlp) (5.9.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacrebleu->lit_nlp) (0.8.10)\n",
            "Requirement already satisfied: lxml in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacrebleu->lit_nlp) (4.9.1)\n",
            "Requirement already satisfied: portalocker in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacrebleu->lit_nlp) (2.6.0)\n",
            "Requirement already satisfied: click in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers==4.1.1) (8.1.3)\n",
            "Requirement already satisfied: joblib in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers==4.1.1) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->lit_nlp) (3.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-metadata->tfds-nightly) (1.57.0)\n",
            "Requirement already satisfied: numba>=0.49 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from umap-learn->lit_nlp) (0.56.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Using cached pynndescent-0.5.8-py3-none-any.whl\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Werkzeug->lit_nlp) (2.1.1)\n",
            "Requirement already satisfied: setuptools<60 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from numba>=0.49->umap-learn->lit_nlp) (59.8.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from numba>=0.49->umap-learn->lit_nlp) (0.39.1)\n",
            "Requirement already satisfied: pywin32>=226 in c:\\users\\asif.raza\\appdata\\roaming\\python\\python310\\site-packages (from portalocker->sacrebleu->lit_nlp) (304)\n",
            "Building wheels for collected packages: tokenizers\n",
            "  Building wheel for tokenizers (pyproject.toml): started\n",
            "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
            "Failed to build tokenizers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [47 lines of output]\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-310\n",
            "      creating build\\lib.win-amd64-cpython-310\\tokenizers\n",
            "      copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\n",
            "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\models\n",
            "      copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\models\n",
            "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\decoders\n",
            "      copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\decoders\n",
            "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\normalizers\n",
            "      copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\normalizers\n",
            "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\pre_tokenizers\n",
            "      copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\pre_tokenizers\n",
            "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\processors\n",
            "      copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\processors\n",
            "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\trainers\n",
            "      copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\trainers\n",
            "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
            "      copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
            "      copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
            "      copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
            "      copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
            "      copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
            "      copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
            "      copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
            "      copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\n",
            "      copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\models\n",
            "      copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\decoders\n",
            "      copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\normalizers\n",
            "      copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\pre_tokenizers\n",
            "      copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\processors\n",
            "      copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\trainers\n",
            "      running build_ext\n",
            "      running build_rust\n",
            "      error: can't find Rust compiler\n",
            "      \n",
            "      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
            "      \n",
            "      To update pip, run:\n",
            "      \n",
            "          pip install --upgrade pip\n",
            "      \n",
            "      and then retry package installation.\n",
            "      \n",
            "      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for tokenizers\n",
            "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
          ]
        }
      ],
      "source": [
        "# Install LIT and transformers packages. The transformers package is needed by the model and dataset we are using.\n",
        "# Replace tensorflow-datasets with the nightly package to get up-to-date dataset paths.\n",
        "!pip uninstall -y tensorflow-datasets\n",
        "!pip install lit_nlp tfds-nightly transformers==4.1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "30l9ZyTjxJjf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "tar: Error opening archive: Failed to open 'sst2_tiny.tar.gz'\n"
          ]
        }
      ],
      "source": [
        "# Fetch the trained model weights\n",
        "!wget https://storage.googleapis.com/what-if-tool-resources/lit-models/sst2_tiny.tar.gz\n",
        "!tar -xvf sst2_tiny.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AWhbAZg57RpB"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Can't load config for './'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './' is the correct path to a directory containing a config.json file",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\asif.raza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\configuration_utils.py:623\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    622\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 623\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_path(\n\u001b[0;32m    624\u001b[0m         config_file,\n\u001b[0;32m    625\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    626\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    627\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    628\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    629\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    630\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    631\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    634\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
            "File \u001b[1;32mc:\\Users\\asif.raza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:299\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39melif\u001b[39;00m urlparse(url_or_filename)\u001b[39m.\u001b[39mscheme \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    298\u001b[0m     \u001b[39m# File, but it doesn't exist.\u001b[39;00m\n\u001b[1;32m--> 299\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfile \u001b[39m\u001b[39m{\u001b[39;00murl_or_filename\u001b[39m}\u001b[39;00m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    300\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     \u001b[39m# Something unknown\u001b[39;00m\n",
            "\u001b[1;31mOSError\u001b[0m: file ./config.json not found",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlit_nlp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexamples\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m glue_models\n\u001b[0;32m      6\u001b[0m datasets \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39msst_dev\u001b[39m\u001b[39m'\u001b[39m: glue\u001b[39m.\u001b[39mSST2Data(\u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m)}\n\u001b[1;32m----> 7\u001b[0m models \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39msst_tiny\u001b[39m\u001b[39m'\u001b[39m: glue_models\u001b[39m.\u001b[39;49mSST2Model(\u001b[39m'\u001b[39;49m\u001b[39m./\u001b[39;49m\u001b[39m'\u001b[39;49m)}\n\u001b[0;32m      9\u001b[0m widget \u001b[39m=\u001b[39m notebook\u001b[39m.\u001b[39mLitWidget(models, datasets, height\u001b[39m=\u001b[39m\u001b[39m800\u001b[39m)\n",
            "File \u001b[1;32md:\\Thesis\\Thesis-II\\XNLP-Thesis\\src\\LIT Code Incorporate\\lit_nlp\\examples\\models\\glue_models.py:500\u001b[0m, in \u001b[0;36mSST2Model.__init__\u001b[1;34m(self, *args, **kw)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m--> 500\u001b[0m   \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    501\u001b[0m       \u001b[39m*\u001b[39margs,\n\u001b[0;32m    502\u001b[0m       text_a_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    503\u001b[0m       text_b_name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    504\u001b[0m       labels\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m    505\u001b[0m       null_label_idx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[0;32m    506\u001b[0m       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
            "File \u001b[1;32md:\\Thesis\\Thesis-II\\XNLP-Thesis\\src\\LIT Code Incorporate\\lit_nlp\\examples\\models\\glue_models.py:61\u001b[0m, in \u001b[0;36mGlueModel.__init__\u001b[1;34m(self, model_name_or_path, **config_kw)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m     58\u001b[0m              model_name_or_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     59\u001b[0m              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_kw):\n\u001b[0;32m     60\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m GlueModelConfig(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_kw)\n\u001b[1;32m---> 61\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_model(model_name_or_path)\n\u001b[0;32m     62\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mLock()\n",
            "File \u001b[1;32md:\\Thesis\\Thesis-II\\XNLP-Thesis\\src\\LIT Code Incorporate\\lit_nlp\\examples\\models\\glue_models.py:66\u001b[0m, in \u001b[0;36mGlueModel._load_model\u001b[1;34m(self, model_name_or_path)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_model\u001b[39m(\u001b[39mself\u001b[39m, model_name_or_path):\n\u001b[0;32m     65\u001b[0m   \u001b[39m\"\"\"Load model. Can be overridden for testing.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mAutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m     67\u001b[0m       model_name_or_path)\n\u001b[0;32m     68\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mconvert_ids_to_tokens(\n\u001b[0;32m     69\u001b[0m       \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer)))\n\u001b[0;32m     70\u001b[0m   model_config \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mAutoConfig\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     71\u001b[0m       model_name_or_path,\n\u001b[0;32m     72\u001b[0m       num_labels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_regression \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mlabels),\n\u001b[0;32m     73\u001b[0m       return_dict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,  \u001b[39m# default for training; overridden for predict\u001b[39;00m\n\u001b[0;32m     74\u001b[0m   )\n",
            "File \u001b[1;32mc:\\Users\\asif.raza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:547\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[39mif\u001b[39;00m config_tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    546\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m--> 547\u001b[0m         config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    548\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[39m=\u001b[39mtrust_remote_code, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    549\u001b[0m         )\n\u001b[0;32m    550\u001b[0m     config_tokenizer_class \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mtokenizer_class\n\u001b[0;32m    551\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoTokenizer\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mauto_map:\n",
            "File \u001b[1;32mc:\\Users\\asif.raza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:725\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    723\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    724\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 725\u001b[0m config_dict, _ \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    726\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    727\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
            "File \u001b[1;32mc:\\Users\\asif.raza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\configuration_utils.py:561\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    559\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    560\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    563\u001b[0m \u001b[39m# That config file may point us toward another config file to use.\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mconfiguration_files\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n",
            "File \u001b[1;32mc:\\Users\\asif.raza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\configuration_utils.py:663\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    657\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWe couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt connect to \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m to load this model, couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find it in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    658\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m the cached files and it looks like \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m is not the path to a directory\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    659\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m containing a \u001b[39m\u001b[39m{\u001b[39;00mconfiguration_file\u001b[39m}\u001b[39;00m\u001b[39m file.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCheckout your internet connection or see how to run the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    660\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m library in offline mode at \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    661\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m--> 663\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    664\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load config for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    665\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    666\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOtherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    667\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontaining a \u001b[39m\u001b[39m{\u001b[39;00mconfiguration_file\u001b[39m}\u001b[39;00m\u001b[39m file\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    668\u001b[0m     )\n\u001b[0;32m    670\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    671\u001b[0m     \u001b[39m# Load config dict\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     config_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_dict_from_json_file(resolved_config_file)\n",
            "\u001b[1;31mOSError\u001b[0m: Can't load config for './'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './' is the correct path to a directory containing a config.json file"
          ]
        }
      ],
      "source": [
        "# Create the LIT widget with the model and dataset to analyze.\n",
        "from lit_nlp import notebook\n",
        "from lit_nlp.examples.datasets import glue\n",
        "from lit_nlp.examples.models import glue_models\n",
        "\n",
        "datasets = {'sst_dev': glue.SST2Data('validation')}\n",
        "models = {'sst_tiny': glue_models.SST2Model('./')}\n",
        "\n",
        "widget = notebook.LitWidget(models, datasets, height=800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GSfs1waBdLd"
      },
      "outputs": [],
      "source": [
        "# Render the widget\n",
        "widget.render()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "LIT in Notebooks",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "f059acb20147c269a8b38bfd65cfb23493aee2ca0d0c37762e34bc8c6dcfae89"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
