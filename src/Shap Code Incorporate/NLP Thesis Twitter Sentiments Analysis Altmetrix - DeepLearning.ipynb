{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxI3upc11OVs"
   },
   "source": [
    "### Data Facts and Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "JRN6_qD21OVs"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Dataset/SentimentAnalysisofTweetsthroughAltmetrics/train.csv')\n",
    "df_test = pd.read_csv('Dataset/SentimentAnalysisofTweetsthroughAltmetrics/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns = [\"Text\", \"Label\"]\n",
    "df_test.columns = [\"Text\", \"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the neutural.\n",
    "df_train= df_train[df_train['Label'] != 0]\n",
    "df_test= df_test[df_test['Label'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "VZwFTz-A1OVt",
    "outputId": "f2517b42-3fa7-4312-9fc7-964a25c9a83f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(470, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "L8TYsYXd1OVt",
    "outputId": "8c536a30-abbe-4141-820e-d6ed3d427d59"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good acronym copper nanotubes Definitely</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GlycemicIndex diet restricted energy effective...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>higher fibre intake partic cereal fibre reduce...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>next life going research copper nanotubes CuNTs</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bean rich diet produces equivalent weight loss...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0           good acronym copper nanotubes Definitely     -1\n",
       "2  GlycemicIndex diet restricted energy effective...      1\n",
       "3  higher fibre intake partic cereal fibre reduce...      1\n",
       "4    next life going research copper nanotubes CuNTs     -1\n",
       "6  Bean rich diet produces equivalent weight loss...      1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yeah paper ebirdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>platform Bioinformatics paper advanced access ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Duan naturally award Best Science Acronym year</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Everything Chinese turns swear word think karma</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dear difficulties finding scientific abbreviat...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0                                  Yeah paper ebirdf      1\n",
       "2  platform Bioinformatics paper advanced access ...      1\n",
       "4     Duan naturally award Best Science Acronym year     -1\n",
       "5    Everything Chinese turns swear word think karma     -1\n",
       "6  dear difficulties finding scientific abbreviat...     -1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "H95cCeRA1OVu",
    "outputId": "576984d9-82e4-4ee8-d7bc-19630be52479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 470 entries, 0 to 730\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Text    470 non-null    object\n",
      " 1   Label   470 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 11.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 201 entries, 0 to 313\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Text    201 non-null    object\n",
      " 1   Label   201 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 4.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "v291MZbH1OVu",
    "outputId": "d9bf5970-1bbc-4738-adcd-6159bb3897b7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <td>470.0</td>\n",
       "      <td>0.029787</td>\n",
       "      <td>1.000621</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count      mean       std  min  25%  50%  75%  max\n",
       "Label  470.0  0.029787  1.000621 -1.0 -1.0  1.0  1.0  1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <td>201.0</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>1.002385</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count      mean       std  min  25%  50%  75%  max\n",
       "Label  201.0  0.014925  1.002385 -1.0 -1.0  1.0  1.0  1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKAInF9Y1OVv"
   },
   "source": [
    "### Data Cleaning / EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "05JkHavx1OVv",
    "outputId": "0c57f6d4-fb8e-4efc-c873-8acd0fb2f367"
   },
   "outputs": [],
   "source": [
    "# ### Checking Missing values in the Data Set and printing the Percentage for Missing Values for Each Columns ###\n",
    "\n",
    "# count = df_train.isnull().sum().sort_values(ascending=False)\n",
    "# percentage = ((df_train.isnull().sum()/len(df_train)*100)).sort_values(ascending=False)\n",
    "# missing_data = pd.concat([count, percentage], axis=1, keys=['Count','Percentage'])\n",
    "\n",
    "# print('Count and percentage of missing values for the columns:')\n",
    "\n",
    "# missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "WPhBSeU71OVv",
    "outputId": "b55cfc7a-97ff-4e80-a253-fcf25a49852c"
   },
   "outputs": [],
   "source": [
    "# ### Checking for the Distribution of Default ###\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# print('Percentage for default\\n')\n",
    "# print(round(df_train.Is_Response.value_counts(normalize=True)*100,2))\n",
    "# round(df_train.Is_Response.value_counts(normalize=True)*100,2).plot(kind='bar')\n",
    "# plt.title('Percentage Distributions by review type')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "4Y1DC1PC1OVw"
   },
   "outputs": [],
   "source": [
    "#Removing columns\n",
    "#df_train.drop(columns = ['User_ID', 'Browser_Used', 'Device_Used'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "FRDzw0zR1OVw"
   },
   "outputs": [],
   "source": [
    "# #This function converts to lower-case, removes square bracket, removes numbers and punctuation\n",
    "# def text_clean_1(text):\n",
    "#     text = text.lower()\n",
    "#     text = re.sub('\\[.*?\\]', '', text)\n",
    "#     text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "#     text = re.sub('\\w*\\d\\w*', '', text)\n",
    "#     return text\n",
    "\n",
    "# cleaned1 = lambda x: text_clean_1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "gA28egw71OVw",
    "outputId": "b715ee77-3b99-4e4d-ed46-f42baf57c77f"
   },
   "outputs": [],
   "source": [
    "# # Apply first level cleaning\n",
    "\n",
    "# # Let's take a look at the updated text\n",
    "# df_train['cleaned_description'] = pd.DataFrame(df_train.Description.apply(cleaned1))\n",
    "# df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "21SR414M1OVx"
   },
   "outputs": [],
   "source": [
    "# # Apply a second round of cleaning\n",
    "# def text_clean_2(text):\n",
    "#     text = re.sub('[‘’“”…]', '', text)\n",
    "#     text = re.sub('\\n', '', text)\n",
    "#     return text\n",
    "\n",
    "# cleaned2 = lambda x: text_clean_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "7bOh84xL1OVx",
    "outputId": "66c63f62-4028-44b6-996a-8b837254cfec"
   },
   "outputs": [],
   "source": [
    "# # Let's take a look at the updated text\n",
    "# df_train['cleaned_description_new'] = pd.DataFrame(df_train['cleaned_description'].apply(cleaned2))\n",
    "# df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IV_train : 470\n",
      "IV_test  : 201\n",
      "DV_train : 470\n",
      "DV_test  : 201\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Independent_var = df_train.cleaned_description_new\n",
    "#Dependent_var = df_train.Is_Response\n",
    "\n",
    "#IV_train, IV_test, DV_train, DV_test = train_test_split(Independent_var, Dependent_var, test_size = 0.1, random_state = 225)\n",
    "\n",
    "IV_train = df_train.Text\n",
    "DV_train = df_train.Label\n",
    "IV_test = df_test.Text\n",
    "DV_test = df_test.Label\n",
    "\n",
    "\n",
    "print('IV_train :', len(IV_train))\n",
    "print('IV_test  :', len(IV_test))\n",
    "print('DV_train :', len(DV_train))\n",
    "print('DV_test  :', len(DV_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnjZY4h51OVx"
   },
   "source": [
    "### Model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model training code is directly from:\n",
    "# https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "\n",
    "'''Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "# Notes\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import pad_sequences\n",
    "from keras_preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build model...\n",
      "Train...\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.4321 - accuracy: 0.7925WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000247111F30A0> and will run it as-is.\n",
      "Cause: generators are not supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000247111F30A0> and will run it as-is.\n",
      "Cause: generators are not supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "782/782 [==============================] - 232s 291ms/step - loss: 0.4321 - accuracy: 0.7925 - val_loss: 0.3693 - val_accuracy: 0.8380\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.3693 - accuracy: 0.8380\n",
      "Test score: 0.3692709803581238\n",
      "Test accuracy: 0.8380399942398071\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train =pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras==2.3.1 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras==2.3.1) (6.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras==2.3.1) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras==2.3.1) (1.23.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras==2.3.1) (1.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\asif.raza\\appdata\\roaming\\python\\python310\\site-packages (from keras==2.3.1) (1.16.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras==2.3.1) (3.7.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\asif.raza\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras==2.3.1) (1.9.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow==1.14.0 (from versions: 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0)\n",
      "ERROR: No matching distribution found for tensorflow==1.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asif.raza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "keras is no longer supported, please use tf.keras instead.\n",
      "Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [49], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m explainer \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39mDeepExplainer(model, x_train[:\u001b[39m100\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[39m# explain the first 10 predictions\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# explaining each prediction requires 2 * background dataset size runs\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m shap_values \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39;49mshap_values(x_test[:\u001b[39m2\u001b[39;49m])\n",
      "File \u001b[1;32md:\\Thesis\\Thesis-II\\XNLP-Thesis\\src\\Shap Code Incorporate\\shap\\explainers\\_deep\\__init__.py:124\u001b[0m, in \u001b[0;36mDeep.shap_values\u001b[1;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshap_values\u001b[39m(\u001b[39mself\u001b[39m, X, ranked_outputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, output_rank_order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m, check_additivity\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m     91\u001b[0m     \u001b[39m\"\"\" Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \n\u001b[0;32m     93\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[39m        were chosen as \"top\".\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplainer\u001b[39m.\u001b[39;49mshap_values(X, ranked_outputs, output_rank_order, check_additivity\u001b[39m=\u001b[39;49mcheck_additivity)\n",
      "File \u001b[1;32md:\\Thesis\\Thesis-II\\XNLP-Thesis\\src\\Shap Code Incorporate\\shap\\explainers\\_deep\\deep_tf.py:311\u001b[0m, in \u001b[0;36mTFDeep.shap_values\u001b[1;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[39m# run attribution computation graph\u001b[39;00m\n\u001b[0;32m    310\u001b[0m feature_ind \u001b[39m=\u001b[39m model_output_ranks[j,i]\n\u001b[1;32m--> 311\u001b[0m sample_phis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mphi_symbolic(feature_ind), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_inputs, joint_input)\n\u001b[0;32m    313\u001b[0m \u001b[39m# assign the attributions to the right part of the output arrays\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X)):\n",
      "File \u001b[1;32md:\\Thesis\\Thesis-II\\XNLP-Thesis\\src\\Shap Code Incorporate\\shap\\explainers\\_deep\\deep_tf.py:371\u001b[0m, in \u001b[0;36mTFDeep.run\u001b[1;34m(self, out, model_inputs, X)\u001b[0m\n\u001b[0;32m    368\u001b[0m         tf_execute\u001b[39m.\u001b[39mrecord_gradient \u001b[39m=\u001b[39m tf_backprop\u001b[39m.\u001b[39mrecord_gradient\n\u001b[0;32m    370\u001b[0m     \u001b[39mreturn\u001b[39;00m final_out\n\u001b[1;32m--> 371\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute_with_overridden_gradients(anon)\n",
      "File \u001b[1;32md:\\Thesis\\Thesis-II\\XNLP-Thesis\\src\\Shap Code Incorporate\\shap\\explainers\\_deep\\deep_tf.py:407\u001b[0m, in \u001b[0;36mTFDeep.execute_with_overridden_gradients\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[39m# define the computation graph for the attribution values using a custom gradient-like computation\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 407\u001b[0m     out \u001b[39m=\u001b[39m f()\n\u001b[0;32m    408\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    409\u001b[0m     \u001b[39m# reinstate the backpropagatable check\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tf_gradients_impl, \u001b[39m\"\u001b[39m\u001b[39m_IsBackpropagatable\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\Thesis\\Thesis-II\\XNLP-Thesis\\src\\Shap Code Incorporate\\shap\\explainers\\_deep\\deep_tf.py:361\u001b[0m, in \u001b[0;36mTFDeep.run.<locals>.anon\u001b[1;34m()\u001b[0m\n\u001b[0;32m    359\u001b[0m shape \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_inputs[i]\u001b[39m.\u001b[39mshape)\n\u001b[0;32m    360\u001b[0m shape[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m--> 361\u001b[0m data \u001b[39m=\u001b[39m X[i]\u001b[39m.\u001b[39;49mreshape(shape)\n\u001b[0;32m    362\u001b[0m v \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant(data, dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_inputs[i]\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    363\u001b[0m inputs\u001b[39m.\u001b[39mappend(v)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "# we use the first 100 training examples as our background dataset to integrate over\n",
    "explainer = shap.DeepExplainer(model, x_train[:100])\n",
    "\n",
    "# explain the first 10 predictions\n",
    "# explaining each prediction requires 2 * background dataset size runs\n",
    "shap_values = explainer.shap_values(x_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the JS visualization code\n",
    "shap.initjs()\n",
    "\n",
    "# transform the indexes to words\n",
    "import numpy as np\n",
    "words = imdb.get_word_index()\n",
    "num2word = {}\n",
    "for w in words.keys():\n",
    "    num2word[words[w]] = w\n",
    "x_test_words = np.stack([np.array(list(map(lambda x: num2word.get(x, \"NONE\"), x_test[i]))) for i in range(10)])\n",
    "\n",
    "# plot the explanation of the first prediction\n",
    "# Note the model is \"multi-output\" because it is rank-2 but only has one column\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][0], x_test_words[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Hotel review sentiment analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "f059acb20147c269a8b38bfd65cfb23493aee2ca0d0c37762e34bc8c6dcfae89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
